{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76572a3",
   "metadata": {},
   "source": [
    "# import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfee05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import statistics as stats\n",
    "import scipy\n",
    "import string as st\n",
    "import lmfit\n",
    "from lmfit import Model\n",
    "from lmfit import models\n",
    "from lmfit.models import *\n",
    "from scipy import io\n",
    "from scipy.fftpack import dct, idct\n",
    "from itertools import combinations\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import inspect\n",
    "import csv\n",
    "\n",
    "#saving each graph \n",
    "savefile = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764a1d9",
   "metadata": {},
   "source": [
    "# define the likelihood ratio calculation function\n",
    "\n",
    "Based on Aitken and Lucy (2004)  \n",
    "https://doi.org/10.1046/j.0035-9254.2003.05271.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c20a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(background, file1, file2):\n",
    "    \n",
    "    off_covar = None\n",
    "\n",
    "    ######################################################\n",
    "    ### some prior calculations on the background data ###\n",
    "    ######################################################\n",
    "\n",
    "    # Get the number of speakers\n",
    "    num_speakers = len(background)\n",
    "\n",
    "    # get the number of variables\n",
    "    num_variables = len(next(iter(background.values())))\n",
    "\n",
    "    # get the number of measurements \n",
    "    num_measur = 0\n",
    "    for speaker in background.values():\n",
    "        first_variable = next(iter(speaker.values()))\n",
    "        num_measur += len(first_variable)\n",
    "\n",
    "    # background mean : overall mean for each variable. \n",
    "\n",
    "    var_all = {}\n",
    "\n",
    "    #first reshape the data to put all values together, regardless of speaker\n",
    "    for speaker, variables in background.items(): \n",
    "        # loop through every variable \n",
    "        for variable, values in variables.items():\n",
    "            # if the new variable has not yet been added to the new dictionary, add it.\n",
    "            var_all.setdefault(variable, [])\n",
    "\n",
    "            # add the values to the coefficient list\n",
    "            var_all[variable].extend(values)\n",
    "\n",
    "    #calculate the mean of each variable in the background data:\n",
    "\n",
    "    background_means = {}\n",
    "\n",
    "    for variable, values in var_all.items():\n",
    "        background_means.setdefault(variable, )\n",
    "        background_means[variable] = np.mean(values)\n",
    "\n",
    "    #mean of each variable for eah of the speakers in the background data: \n",
    "\n",
    "    var_means_per_speaker = {}\n",
    "\n",
    "    for speaker,var in background.items():\n",
    "        var_means_per_speaker[speaker] = []\n",
    "        for variable, values in background[speaker].items():\n",
    "            var_means_per_speaker[speaker].append(np.mean(values))\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ### within group covariance matrix ###\n",
    "    ######################################\n",
    "\n",
    "    #calculated differently to make use of the cov function from the numpy package. \n",
    "\n",
    "    #create a covariance matrix for each speaker. \n",
    "                        \n",
    "    cov_matrices = {}\n",
    "    for speaker, values in background.items():\n",
    "        speaker_data = np.vstack(list(values.values()))\n",
    "\n",
    "        numeric_data = speaker_data.astype(np.number)\n",
    "        cov_matrices[speaker] = np.cov(numeric_data)\n",
    "\n",
    "    #adding all matrices\n",
    "\n",
    "    matrices = [np.array(matrix) for matrix in cov_matrices.values()]\n",
    "    sum_matrices = sum(matrix[:matrix.shape[0], :matrix.shape[1]] for matrix in matrices)\n",
    "\n",
    "    # divide by the number of speakers.\n",
    "    within_group = sum_matrices/num_speakers\n",
    "\n",
    "    #######################################\n",
    "    ### between group covariance matrix ###\n",
    "    #######################################\n",
    "\n",
    "    #according to the changes introduced by Morrison to accomodate for unequal n across speakers. \n",
    "\n",
    "    #calculate the first part of the group covariance matrix equation.\n",
    "    Sstar= np.cov(np.array(list(var_means_per_speaker.values())), rowvar=False)\n",
    "\n",
    "    #final calculations\n",
    "    between_group_cov = Sstar - (sum_matrices/num_measur)\n",
    "\n",
    "    #between group inverse covariance\n",
    "    betw_inv = np.linalg.inv(between_group_cov)\n",
    "\n",
    "    #between group square root\n",
    "    betw_sqrt = scipy.linalg.sqrtm(between_group_cov)\n",
    "\n",
    "    ####################################\n",
    "    ### calulations on offender data ###\n",
    "    ####################################\n",
    "\n",
    "    # number of measures\n",
    "    off_num_measures = len(next(iter(file1.values())))\n",
    "\n",
    "    # offender mean\n",
    "    off_mean = []\n",
    "    for variable, values in file1.items():\n",
    "        off_mean.append(stats.mean(file1[variable])) \n",
    "\n",
    "    #offender covariance matrix \n",
    "\n",
    "    off_covar= [[value / off_num_measures for value in row] for row in within_group]\n",
    "        \n",
    "    #offender inverse covariance matrix.\n",
    "    off_inv = np.linalg.inv(off_covar)\n",
    "\n",
    "    #offender square root \n",
    "    off_sqrt = scipy.linalg.sqrtm(off_covar)\n",
    "\n",
    "    # offender square root inverse\n",
    "    off_sqrt_inv = np.linalg.inv(off_sqrt)\n",
    "\n",
    "     #################################\n",
    "     ### suspect data calculations ###\n",
    "    #################################\n",
    "\n",
    "    # number of measures\n",
    "    suspect_num_measures = len(next(iter(file2.values())))\n",
    "\n",
    "    # suspect mean\n",
    "    suspect_mean = []\n",
    "    for variable, values in file2.items():\n",
    "        suspect_mean.append(stats.mean(file2[variable])) \n",
    "\n",
    "    #suspect covariance matrix \n",
    "\n",
    "    suspect_covar = [[value / suspect_num_measures for value in row] for row in within_group]\n",
    "    suspect_covar = np.array(suspect_covar)\n",
    "\n",
    "    #suspect inverse covariance matrix.\n",
    "    suspect_inv = np.linalg.inv(suspect_covar)\n",
    "\n",
    "    #suspect square root \n",
    "    suspect_sqrt = scipy.linalg.sqrtm(suspect_covar)\n",
    "\n",
    "    # suspect square root inverse\n",
    "    suspect_sqrt_inv = np.linalg.inv(suspect_sqrt)\n",
    "\n",
    "    ###########################\n",
    "    ### smoothing parameter ###\n",
    "    ###########################\n",
    "    \n",
    "    # as given by Silverman(1986)\n",
    "\n",
    "    smooth_param= ((4/(2*num_variables+1))**(1/(num_variables+4)) \n",
    "                   * num_speakers**-(1/(num_variables+4)))\n",
    "\n",
    "    ####################################\n",
    "    ### a few other pre-calculations ###\n",
    "    ####################################\n",
    "\n",
    "    kernel = smooth_param**2 * between_group_cov\n",
    "\n",
    "    inv_kernel = np.linalg.inv(kernel)\n",
    "\n",
    "    kernel_typ = 0\n",
    "    dist_back_off = 0\n",
    "    dist_back_suspect = 0 \n",
    "    suspect_off_mean_typ = 0\n",
    "\n",
    "    suspect_off_mean_diff = np.subtract(suspect_mean, off_mean)\n",
    "    suspect_off_mean_typ = np.linalg.solve(off_inv \n",
    "                                           + suspect_inv, np.linalg.solve(off_covar, off_mean) \n",
    "                                           + np.linalg.solve(suspect_covar, suspect_mean))\n",
    "    \n",
    "    for speaker, values in var_means_per_speaker.items():\n",
    "        typicality = np.subtract(suspect_off_mean_typ, values)\n",
    "\n",
    "        #kernel density at typicality.\n",
    "        kernel_typ += (np.exp(-0.5 * typicality.T \n",
    "                             @ np.linalg.inv(np.linalg.inv(off_inv + suspect_inv) + kernel) \n",
    "                             @ typicality))\n",
    "\n",
    "    #calculations of the distance between background and offender data\n",
    "    dist_back_off += (np.exp(-0.5 * np.subtract(off_mean, values).T \n",
    "                                 @ np.linalg.inv(off_covar + kernel) \n",
    "                                 @ np.subtract(off_mean, values)))\n",
    "\n",
    "    #calculations of the distance between background and suspect data\n",
    "    dist_back_suspect += (np.exp(-0.5 * np.subtract(off_mean, values).T\n",
    "                                 @ np.linalg.inv(off_covar + kernel) \n",
    "                                 @ np.subtract(off_mean, values)))\n",
    "\n",
    "    #################\n",
    "    ### numerator ###\n",
    "    #################\n",
    "\n",
    "    numerator = ((2 * np.pi) **(-num_variables) \n",
    "                     * np.linalg.det(off_sqrt_inv) \n",
    "                     * np.linalg.det(suspect_sqrt_inv)\n",
    "                     * 1/np.sqrt(np.abs(np.linalg.det(between_group_cov))) \n",
    "                     * (num_speakers * smooth_param ** num_variables) ** (-1) \n",
    "                     * np.abs(np.linalg.det(off_inv + suspect_inv + inv_kernel)) ** (-0.5) \n",
    "                     * np.exp(-0.5 * \n",
    "                              suspect_off_mean_diff.T \n",
    "                              @ np.linalg.inv(suspect_covar + off_covar) \n",
    "                              @ suspect_off_mean_diff) \n",
    "                     * kernel_typ)\n",
    "\n",
    "    ###################\n",
    "    ### denominator ###\n",
    "    ###################\n",
    "\n",
    "    denominator = ( (2*np.pi)**-num_variables * 1/abs(np.linalg.det(between_group_cov))\n",
    "                       * (num_speakers * smooth_param**num_variables)**-2 \n",
    "\n",
    "                       * abs(np.linalg.det(suspect_sqrt_inv))\n",
    "                       * abs(np.linalg.det(suspect_inv + inv_kernel))**-0.5\n",
    "                       * dist_back_suspect\n",
    "\n",
    "                       * abs(np.linalg.det(off_sqrt_inv))\n",
    "                       * abs(np.linalg.det(off_inv + inv_kernel))**-0.5\n",
    "                       * dist_back_off)\n",
    "\n",
    "    ########################\n",
    "    ### likelihood ratio ###\n",
    "    ########################\n",
    "\n",
    "    likelihood_ratio= numerator / denominator\n",
    "\n",
    "    return(likelihood_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7a042",
   "metadata": {},
   "source": [
    "# upload and sort the data\n",
    "first uploading the data (needs to be saved in the same folder as this notebook under the name 'formant_diphthongs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb69341b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Open the file dialog window\n",
    "input_file = filedialog.askopenfilename()\n",
    "\n",
    "if not input_file.endswith(\".csv\"):\n",
    "    print(\"Invalid file format. Please provide a CSV file.\")\n",
    "\n",
    "raw_df = pd.read_csv(input_file)\n",
    "#print(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2696cb",
   "metadata": {},
   "source": [
    "The markers for the diphthongs are not Python-friendly and need to be changed. The new marker attempt to stay close to the german written form, alternatively, characters close to their IPA form can be chosen. The rest of the Python script will be using exclusively latin characters, to avoid confusion and problems with Python.\\\n",
    "The table gets then grouped by the column \"file\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774ed384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d={'\\\\ct\\\\at': 'or','e\\\\at': 'er',\n",
    "   '\\\\ef\\\\at': 'e_r', 'e\\\\ri\\\\at': 'e_ra',\n",
    "   'a\\\\at': 'ar', '\\\\e-h\\\\yc': 'ehu',\n",
    "   '\\\\ef\\\\atje': 'e_rje', 'ah\\\\ef\\\\at': 'ahea',\n",
    "   '\\\\hs\\\\at': 'ua', '\\\\e-je': 'eje',\n",
    "   'i\\\\at': 'ir', '\\\\cti': 'oi',\n",
    "   'a\\\\hs': 'au', '\\\\e-\\\\ri\\\\at': 'era',\n",
    "   '\\\\yc\\\\at': 'u_a', 'e\\\\atje': 'erje',\n",
    "   'ah\\\\at': 'ahr', '\\\\ef\\\\ar': 'e_a',\n",
    "   'e\\\\ri\\\\e-': 'e_re', '\\\\efje': 'e_je',\n",
    "   '\\\\e-j\\\\ef': 'eje_', '\\\\atje': 'rje'}\n",
    "betterID = raw_df.replace(d)\n",
    "\n",
    "#drop columns containing NaN values, prevent the code breaking if f1 is not calculated.\n",
    "betterID = betterID.dropna(axis=1)\n",
    "#print (betterID) \n",
    "\n",
    "filegroup = betterID.groupby([\"file\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a652f",
   "metadata": {},
   "source": [
    "# curve fitting\n",
    "\n",
    "Each analysis iterates through the groups \"file\". Within each file, additional groups get created by diphthongs and can be filtered. This makes the analysis of different diphthong easier. Here only the diphthongs with the marker \"ai\" (as in \"beide\") is analysed.\n",
    "\n",
    "The following cell uses a 3rd degree polynomial to fit the diphthongs. The structure functions as follow:\n",
    "- a first iteration through each document (using 'filegroup')\n",
    "- a grouping by diphthongs and the dropping of the first two columns. \n",
    "- a grouping by formants, using the columns names.\n",
    "- the curve fitting of each line within each formant group. \n",
    "\n",
    "For testing, it is possible to prints out a representation of the raw data against the fitting. The graphic shows each fitted curve as a line of darker color and the raw data as lighter points. Each formant is also given a separate colour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2215452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### calculating and displaying the curve fitting with polynomials 3rd degree: ####\n",
    "\n",
    "### creating the result dict \n",
    "aip3res = {file: {'f1': {'c0': [], 'c1': [], 'c2': [], 'c3': []},\n",
    "                  'f2': {'c0': [], 'c1': [], 'c2': [], 'c3': []},\n",
    "                  'f3': {'c0': [], 'c1': [], 'c2': [], 'c3': []}}\n",
    "           for file, _ in filegroup}\n",
    "\n",
    "### some variables that need to be defined beforehand\n",
    "\n",
    "### plot parameters ###\n",
    "xaxis = np.arange(0,101,5)\n",
    "x = xaxis\n",
    "#plot = plt.figure()\n",
    "#f1plot = plt.ylim(0,4000)\n",
    "\n",
    "# regex search\n",
    "aipattern = re.compile('ai')\n",
    "\n",
    " ##### p3 model and parameters #####\n",
    "p3mod = PolynomialModel(3)\n",
    "p3params = p3mod.make_params(c0=650, c1=1, c2=1, c3=1)\n",
    "\n",
    "# Iterate over each file\n",
    "for file, data in betterID.groupby([\"file\"]):\n",
    "    # Get the group of the current file\n",
    "    currentfile = filegroup.get_group((file))\n",
    "    \n",
    "    # Iterate over the diphthongs\n",
    "    for dipht, data in currentfile.groupby(['diphthong']):\n",
    "        # convert the diphthong label to a Regex string\n",
    "        string = ''.join(dipht)\n",
    "        # only work with the labels \"ai\"\n",
    "        if aipattern.search(string):\n",
    "            # Group the data of the current diphthong and drop the \"file\" and \"diphthong\" columns\n",
    "            aidipht = data.drop(['file', 'diphthong'], axis=1).groupby(lambda x: re.search(\"f[1-3]\", x).group(), axis=1)\n",
    "            \n",
    "            # Iterate over the 3 formants\n",
    "            for formant in ['f1', 'f2', 'f3']:\n",
    "                # Get the group of the current formant\n",
    "                fgroup = aidipht.get_group((formant))\n",
    "                \n",
    "                # Iterate over rows of the current formant group\n",
    "                for row, data in fgroup.iterrows():\n",
    "                    # Get the values of the current row as a list and fit a model to them\n",
    "                    f = fgroup.loc[row, :].values.flatten().tolist()\n",
    "                    p3fit = p3mod.fit(f, p3params, x=xaxis)\n",
    "                    \n",
    "                    # Plot the best fit and the raw data points (best fit as darker line and raw data points as light dots)\n",
    "                    #plot = plt.plot(x, p3fit.best_fit, '-', label=formant, color='red' if formant == 'f1' else 'b' if formant == 'f2' else 'orange')\n",
    "                    #plt.plot(x, f, '.', label=formant, color='tomato' if formant == 'f1' else 'lightskyblue' if formant == 'f2' else 'khaki')\n",
    "                    \n",
    "                    # append the results to a dictionary\n",
    "                    for c in range(4):\n",
    "                        aip3res[file][formant][f'c{c}'].append(p3fit.params[f'c{c}'].value)\n",
    "                        \n",
    "    #plt.suptitle(file) \n",
    "    #plt.ylim(0,3400)\n",
    "    if savefile == True :\n",
    "        plt.savefig(file+'_005_f2_f3.pdf')\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b0ad8",
   "metadata": {},
   "source": [
    "The following part modifies the structure of the dictionary and keeps only two levels: file and variables (instead of two levels for the variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5255f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove one level of variable from the dictionary.\n",
    "\n",
    "# creating the new dictionary\n",
    "speaker_dict = {}\n",
    "\n",
    "for file, formants in aip3res.items():\n",
    "    \n",
    "    # if the speaker has not yet been added to the new dictionary, add it.\n",
    "    speaker_dict.setdefault(file, {})\n",
    "    \n",
    "    # loop through every formant inside each recording (second level)   \n",
    "    for formant_name, coeffs in formants.items():\n",
    "        \n",
    "        # loop through every coefficient inside each formant (third level)\n",
    "        for coeff, values in coeffs.items():\n",
    "            \n",
    "            # create a new variable name in the form: formant_coefficient\n",
    "            form_coeffs = formant_name + '_' + coeff\n",
    "            # if the new variable has not yet been added to the new dictionary, add it.\n",
    "            speaker_dict[file].setdefault(form_coeffs, [])\n",
    "            \n",
    "            # add the values to the coefficient list\n",
    "            speaker_dict[file][form_coeffs].extend(values)\n",
    "\n",
    "#print(speaker_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da901f0",
   "metadata": {},
   "source": [
    "# apply the MVKD function\n",
    "The following chunk of code loops accross each possible pair from the list of available recordings. The MVKD function then is applied to each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01340fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### attempt at groups (take two recordings out of the groups, loop through every combination)\n",
    "\n",
    "c =list(combinations(speaker_dict,2)) # get all possible pairs form the list of all files. \n",
    "results = {}\n",
    "pattern = r'([A-Za-z]+)(\\d+)'\n",
    "\n",
    "counter = 0 \n",
    "\n",
    "for fil1, fil2 in c: # iterate through each pair of file \n",
    "    regex1 = re.compile(fil1) # compile into regex format\n",
    "    regex2 = re.compile(fil2)\n",
    "    background = {}\n",
    "    file1 = {}\n",
    "    file2 = {}\n",
    "    background_sorted = {}\n",
    "    for file in speaker_dict: # for each file in the result dict\n",
    "        fil = ''.join(file) # compile file name for regex search \n",
    "        if regex1.search(fil): # if the current file correspond to file 1\n",
    "            for variable, value in speaker_dict[fil].items():\n",
    "                file1[variable] = speaker_dict[fil][variable]# append the results\n",
    "        elif regex2.search(fil): # if the current file correspond to file 2\n",
    "            for variable, value in speaker_dict[fil].items():\n",
    "                file2[variable] = speaker_dict[fil][variable]\n",
    "        elif regex1.search(fil) and regex2.search(fil): # file1 and file2 should always be different\n",
    "            print(\"both files have the same name.\")\n",
    "        else: # the other files are added to the background data.\n",
    "            background[fil] = speaker_dict[fil]\n",
    "    \n",
    "    # combine the recording per speaker, regardless of date. \n",
    "    \n",
    "    for file, variables in background.items():\n",
    "    # make sure that the names follow the naming convention (letters then numbers) \n",
    "        match = re.match(pattern, file)\n",
    "\n",
    "        # otherwise raise an error\n",
    "        if not match:\n",
    "            print(file, 'does not follow the pattern letters+numbers')\n",
    "\n",
    "        # get the name of the speaker (only the letters)\n",
    "        speaker_name = match.group(1)\n",
    "        \n",
    "        background_sorted[speaker_name] = {}\n",
    "        \n",
    "        for variable, values in variables.items():\n",
    "\n",
    "            # if the new variable has not yet been added to the new dictionary, add it.\n",
    "            background_sorted[speaker_name].setdefault(variable, [])\n",
    "\n",
    "            # add the values to the coefficient list\n",
    "            background_sorted[speaker_name][variable].extend(values)\n",
    "\n",
    "        # if the speaker has not yet been added to the new dictionary, add it.\n",
    "        background_sorted.setdefault(speaker_name, {})\n",
    "    \n",
    "    #that's the level at which the sorting and the MVKD function will be used. \n",
    "    #print(fil1, fil2, \":\\n\", likelihood_ratio(background,file1,file2))\n",
    "    \n",
    "    pair = fil1 + \"_\" + fil2\n",
    "    results[pair] = likelihood_ratio(background,file1,file2)\n",
    "\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf6f2e",
   "metadata": {},
   "source": [
    "The likelihood ratios have been gathered in a dictionary and get saved in a csv table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8275cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory path of the script\n",
    "script_dir = os.path.dirname(os.path.abspath(inspect.getframeinfo(inspect.currentframe()).filename))\n",
    "\n",
    "# Specify the file path for saving the dictionary\n",
    "file_path = os.path.join(script_dir, 'LRs_results.csv')\n",
    "\n",
    "with open('LRs_results.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for key, value in results.items():\n",
    "        writer.writerow([key, value])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
